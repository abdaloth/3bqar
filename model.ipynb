{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems = np.load('normalized_poem_text.npy')\n",
    "raw_text = '\\n'.join(poems)\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the datatset:  46468836\n",
      "unique charachter count:  40\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Number of characters in the datatset: \", n_chars)\n",
    "print(\"unique charachter count: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences:  46468794\n",
      "Wall time: 7min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seq_length = 42 # play with this number\n",
    "step_size = 1 # and this number\n",
    "dataX = [] # list of sentences of length seq_length\n",
    "dataY = [] # list of charachters following that sentence\n",
    "for i in range(0, n_chars - seq_length, step_size):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_seq = len(dataX)\n",
    "print(\"number of sequences: \", n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = np.array(dataX)\n",
    "dataY = np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "# then normalize it\n",
    "chunked_dataX = np.array_split(dataX, 114)\n",
    "for chunk in chunked_dataX:\n",
    "    chunk = np.reshape(chunk, (int(n_seq/114), seq_length, 1))\n",
    "    chunk = chunk / float(n_vocab)\n",
    "\n",
    "X = np.concatenate(chunked_dataX, axis=0)\n",
    "X = np.reshape(X, (n_seq, seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save so you don't have to go through that again\n",
    "np.save('input_sequences', X)\n",
    "np.save('output_sequences', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "X = np.load('input_sequences.npy')\n",
    "y = np.load('output_sequences.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the LSTM model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(256))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.3, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01) # TODO: experiment with dropout and learning rates\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer= optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "w_path=\"weights/weights-epoch-{epoch:02d}-loss-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(w_path, monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "46468608/46468794 [============================>.] - ETA: 0s - loss: 3.0450Epoch 00001: loss improved from inf to 3.04495, saving model to weights/weights-epoch-01-loss-3.0450.hdf5\n",
      "46468794/46468794 [==============================] - 13537s 291us/step - loss: 3.0450\n",
      "Epoch 2/20\n",
      "46468608/46468794 [============================>.] - ETA: 0s - loss: 2.8399Epoch 00002: loss improved from 3.04495 to 2.83987, saving model to weights/weights-epoch-02-loss-2.8399.hdf5\n",
      "46468794/46468794 [==============================] - 13514s 291us/step - loss: 2.8399\n",
      "Epoch 3/20\n",
      "46468608/46468794 [============================>.] - ETA: 0s - loss: 2.8597Epoch 00003: loss did not improve\n",
      "46468794/46468794 [==============================] - 13524s 291us/step - loss: 2.8597\n",
      "Epoch 4/20\n",
      "46468608/46468794 [============================>.] - ETA: 0s - loss: 2.9085Epoch 00004: loss did not improve\n",
      "46468794/46468794 [==============================] - 13522s 291us/step - loss: 2.9085\n",
      "Epoch 5/20\n",
      "46468608/46468794 [============================>.] - ETA: 0s - loss: 2.9566Epoch 00005: loss did not improve\n",
      "46468794/46468794 [==============================] - 13433s 289us/step - loss: 2.9566\n",
      "Epoch 6/20\n",
      "20916224/46468794 [============>.................] - ETA: 2:02:25 - loss: 2.9625"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=512, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "chars = # TODO: save char list externally and load it here\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = ''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.3, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.load_weights(filename)\n",
    "optimizer = RMSprop(lr=0.01) # TODO: experiment with dropout and learning rates\n",
    "model.compile(loss='categorical_crossentropy', optimizer= optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = ''\n",
    "sequence = [char_to_int[char] for char in seed]\n",
    "gen_text = ''\n",
    "print('Seed:\\n')\n",
    "print(seed)\n",
    "# generate characters\n",
    "for i in range(140):\n",
    "    x = np.reshape(sequence, (1, len(sequence), 1))\n",
    "    x = x / len(chars)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction) # returns the index of the predicted char\n",
    "    gen_text += int_to_char[index]\n",
    "    sequence.append(index)\n",
    "    sequence = sequence[1:len(sequence)]\n",
    "\n",
    "print(gen_text)\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
