{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poems = np.load('dataset/normalized_poem_text_no_newline.npy')\n",
    "raw_text = ' '.join(poems)\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  150000\n",
      "Total Vocab:  39\n"
     ]
    }
   ],
   "source": [
    "test_text = raw_text[:150000] # taking a small subset to test on my weak machine \n",
    "n_chars = len(test_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  149900\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100 # play with this number\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = test_text[i:i + seq_length]\n",
    "    seq_out = test_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.8533Epoch 00000: loss improved from inf to 2.85330, saving model to weights-improvement-00-2.8533.hdf5\n",
      "149900/149900 [==============================] - 1896s - loss: 2.8533  \n",
      "Epoch 2/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.6447Epoch 00001: loss improved from 2.85330 to 2.64465, saving model to weights-improvement-01-2.6447.hdf5\n",
      "149900/149900 [==============================] - 1859s - loss: 2.6447  \n",
      "Epoch 3/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.5794Epoch 00002: loss improved from 2.64465 to 2.57938, saving model to weights-improvement-02-2.5794.hdf5\n",
      "149900/149900 [==============================] - 1850s - loss: 2.5794  \n",
      "Epoch 4/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.5362Epoch 00003: loss improved from 2.57938 to 2.53613, saving model to weights-improvement-03-2.5361.hdf5\n",
      "149900/149900 [==============================] - 1851s - loss: 2.5361  \n",
      "Epoch 5/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.5021Epoch 00004: loss improved from 2.53613 to 2.50213, saving model to weights-improvement-04-2.5021.hdf5\n",
      "149900/149900 [==============================] - 1856s - loss: 2.5021  \n",
      "Epoch 6/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.4715Epoch 00005: loss improved from 2.50213 to 2.47152, saving model to weights-improvement-05-2.4715.hdf5\n",
      "149900/149900 [==============================] - 1851s - loss: 2.4715  \n",
      "Epoch 7/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.4382Epoch 00006: loss improved from 2.47152 to 2.43822, saving model to weights-improvement-06-2.4382.hdf5\n",
      "149900/149900 [==============================] - 1855s - loss: 2.4382  \n",
      "Epoch 8/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.4112Epoch 00007: loss improved from 2.43822 to 2.41119, saving model to weights-improvement-07-2.4112.hdf5\n",
      "149900/149900 [==============================] - 1858s - loss: 2.4112  \n",
      "Epoch 9/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.3817Epoch 00008: loss improved from 2.41119 to 2.38167, saving model to weights-improvement-08-2.3817.hdf5\n",
      "149900/149900 [==============================] - 1852s - loss: 2.3817  \n",
      "Epoch 10/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.3534Epoch 00009: loss improved from 2.38167 to 2.35341, saving model to weights-improvement-09-2.3534.hdf5\n",
      "149900/149900 [==============================] - 1851s - loss: 2.3534  \n",
      "Epoch 11/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.3266Epoch 00010: loss improved from 2.35341 to 2.32657, saving model to weights-improvement-10-2.3266.hdf5\n",
      "149900/149900 [==============================] - 1853s - loss: 2.3266  \n",
      "Epoch 12/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.2980Epoch 00011: loss improved from 2.32657 to 2.29798, saving model to weights-improvement-11-2.2980.hdf5\n",
      "149900/149900 [==============================] - 1853s - loss: 2.2980  \n",
      "Epoch 13/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.2692Epoch 00012: loss improved from 2.29798 to 2.26921, saving model to weights-improvement-12-2.2692.hdf5\n",
      "149900/149900 [==============================] - 1852s - loss: 2.2692  \n",
      "Epoch 14/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.2420Epoch 00013: loss improved from 2.26921 to 2.24200, saving model to weights-improvement-13-2.2420.hdf5\n",
      "149900/149900 [==============================] - 1853s - loss: 2.2420  \n",
      "Epoch 15/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.2157Epoch 00014: loss improved from 2.24200 to 2.21563, saving model to weights-improvement-14-2.2156.hdf5\n",
      "149900/149900 [==============================] - 1853s - loss: 2.2156  \n",
      "Epoch 16/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.1896Epoch 00015: loss improved from 2.21563 to 2.18966, saving model to weights-improvement-15-2.1897.hdf5\n",
      "149900/149900 [==============================] - 1873s - loss: 2.1897  \n",
      "Epoch 17/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.1660Epoch 00016: loss improved from 2.18966 to 2.16600, saving model to weights-improvement-16-2.1660.hdf5\n",
      "149900/149900 [==============================] - 1853s - loss: 2.1660  \n",
      "Epoch 18/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.1388Epoch 00017: loss improved from 2.16600 to 2.13886, saving model to weights-improvement-17-2.1389.hdf5\n",
      "149900/149900 [==============================] - 1852s - loss: 2.1389  \n",
      "Epoch 19/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.1211Epoch 00018: loss improved from 2.13886 to 2.12117, saving model to weights-improvement-18-2.1212.hdf5\n",
      "149900/149900 [==============================] - 1853s - loss: 2.1212  \n",
      "Epoch 20/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.1127Epoch 00019: loss improved from 2.12117 to 2.11267, saving model to weights-improvement-19-2.1127.hdf5\n",
      "149900/149900 [==============================] - 1854s - loss: 2.1127  \n",
      "Epoch 21/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.0777Epoch 00020: loss improved from 2.11267 to 2.07773, saving model to weights-improvement-20-2.0777.hdf5\n",
      "149900/149900 [==============================] - 1853s - loss: 2.0777  \n",
      "Epoch 22/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.0663Epoch 00021: loss improved from 2.07773 to 2.06637, saving model to weights-improvement-21-2.0664.hdf5\n",
      "149900/149900 [==============================] - 1854s - loss: 2.0664  \n",
      "Epoch 23/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.0504Epoch 00022: loss improved from 2.06637 to 2.05034, saving model to weights-improvement-22-2.0503.hdf5\n",
      "149900/149900 [==============================] - 1854s - loss: 2.0503  \n",
      "Epoch 24/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.0364Epoch 00023: loss improved from 2.05034 to 2.03638, saving model to weights-improvement-23-2.0364.hdf5\n",
      "149900/149900 [==============================] - 1854s - loss: 2.0364  \n",
      "Epoch 25/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.0220Epoch 00024: loss improved from 2.03638 to 2.02197, saving model to weights-improvement-24-2.0220.hdf5\n",
      "149900/149900 [==============================] - 1854s - loss: 2.0220  \n",
      "Epoch 26/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 2.0103Epoch 00025: loss improved from 2.02197 to 2.01033, saving model to weights-improvement-25-2.0103.hdf5\n",
      "149900/149900 [==============================] - 1854s - loss: 2.0103  \n",
      "Epoch 27/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9983Epoch 00026: loss improved from 2.01033 to 1.99824, saving model to weights-improvement-26-1.9982.hdf5\n",
      "149900/149900 [==============================] - 1856s - loss: 1.9982  \n",
      "Epoch 28/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9898Epoch 00027: loss improved from 1.99824 to 1.98979, saving model to weights-improvement-27-1.9898.hdf5\n",
      "149900/149900 [==============================] - 1862s - loss: 1.9898  \n",
      "Epoch 29/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9774Epoch 00028: loss improved from 1.98979 to 1.97746, saving model to weights-improvement-28-1.9775.hdf5\n",
      "149900/149900 [==============================] - 1855s - loss: 1.9775  \n",
      "Epoch 30/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9697Epoch 00029: loss improved from 1.97746 to 1.96965, saving model to weights-improvement-29-1.9696.hdf5\n",
      "149900/149900 [==============================] - 1886s - loss: 1.9696  \n",
      "Epoch 31/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9593Epoch 00030: loss improved from 1.96965 to 1.95933, saving model to weights-improvement-30-1.9593.hdf5\n",
      "149900/149900 [==============================] - 1869s - loss: 1.9593  \n",
      "Epoch 32/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9538Epoch 00031: loss improved from 1.95933 to 1.95373, saving model to weights-improvement-31-1.9537.hdf5\n",
      "149900/149900 [==============================] - 1895s - loss: 1.9537  \n",
      "Epoch 33/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9493Epoch 00032: loss improved from 1.95373 to 1.94929, saving model to weights-improvement-32-1.9493.hdf5\n",
      "149900/149900 [==============================] - 1851s - loss: 1.9493  \n",
      "Epoch 34/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9381Epoch 00033: loss improved from 1.94929 to 1.93818, saving model to weights-improvement-33-1.9382.hdf5\n",
      "149900/149900 [==============================] - 1872s - loss: 1.9382  \n",
      "Epoch 35/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9339Epoch 00034: loss improved from 1.93818 to 1.93390, saving model to weights-improvement-34-1.9339.hdf5\n",
      "149900/149900 [==============================] - 1922s - loss: 1.9339  \n",
      "Epoch 36/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9281Epoch 00035: loss improved from 1.93390 to 1.92813, saving model to weights-improvement-35-1.9281.hdf5\n",
      "149900/149900 [==============================] - 1916s - loss: 1.9281  \n",
      "Epoch 37/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9276Epoch 00036: loss improved from 1.92813 to 1.92755, saving model to weights-improvement-36-1.9276.hdf5\n",
      "149900/149900 [==============================] - 1916s - loss: 1.9276  \n",
      "Epoch 38/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9221Epoch 00037: loss improved from 1.92755 to 1.92205, saving model to weights-improvement-37-1.9221.hdf5\n",
      "149900/149900 [==============================] - 1914s - loss: 1.9221  \n",
      "Epoch 39/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9155Epoch 00038: loss improved from 1.92205 to 1.91548, saving model to weights-improvement-38-1.9155.hdf5\n",
      "149900/149900 [==============================] - 1915s - loss: 1.9155  \n",
      "Epoch 40/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9127Epoch 00039: loss improved from 1.91548 to 1.91270, saving model to weights-improvement-39-1.9127.hdf5\n",
      "149900/149900 [==============================] - 1907s - loss: 1.9127  \n",
      "Epoch 41/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9064Epoch 00040: loss improved from 1.91270 to 1.90644, saving model to weights-improvement-40-1.9064.hdf5\n",
      "149900/149900 [==============================] - 1856s - loss: 1.9064  \n",
      "Epoch 42/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.9013Epoch 00041: loss improved from 1.90644 to 1.90130, saving model to weights-improvement-41-1.9013.hdf5\n",
      "149900/149900 [==============================] - 1865s - loss: 1.9013  \n",
      "Epoch 43/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.8996Epoch 00042: loss improved from 1.90130 to 1.89956, saving model to weights-improvement-42-1.8996.hdf5\n",
      "149900/149900 [==============================] - 1854s - loss: 1.8996  \n",
      "Epoch 44/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.8951Epoch 00043: loss improved from 1.89956 to 1.89506, saving model to weights-improvement-43-1.8951.hdf5\n",
      "149900/149900 [==============================] - 1853s - loss: 1.8951  \n",
      "Epoch 45/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.8935Epoch 00044: loss improved from 1.89506 to 1.89353, saving model to weights-improvement-44-1.8935.hdf5\n",
      "149900/149900 [==============================] - 1852s - loss: 1.8935  \n",
      "Epoch 46/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.8903Epoch 00045: loss improved from 1.89353 to 1.89028, saving model to weights-improvement-45-1.8903.hdf5\n",
      "149900/149900 [==============================] - 1855s - loss: 1.8903  \n",
      "Epoch 47/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.8848Epoch 00046: loss improved from 1.89028 to 1.88477, saving model to weights-improvement-46-1.8848.hdf5\n",
      "149900/149900 [==============================] - 1859s - loss: 1.8848  \n",
      "Epoch 48/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.8819Epoch 00047: loss improved from 1.88477 to 1.88191, saving model to weights-improvement-47-1.8819.hdf5\n",
      "149900/149900 [==============================] - 1864s - loss: 1.8819  \n",
      "Epoch 49/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.8832Epoch 00048: loss did not improve\n",
      "149900/149900 [==============================] - 1863s - loss: 1.8832  \n",
      "Epoch 50/50\n",
      "149888/149900 [============================>.] - ETA: 0s - loss: 1.8790Epoch 00049: loss improved from 1.88191 to 1.87903, saving model to weights-improvement-49-1.8790.hdf5\n",
      "149900/149900 [==============================] - 1865s - loss: 1.8790  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0b44700748>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"weights-improvement-49-1.8790.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  أهكذا أبدا تمضي أمانينا  نطوي الحياة وليل الموت يطوينا  تجري بنا سفن الأعمار ماخرة  بحر الوجود ولا  \"\n",
      "\n",
      "تركر أم يحر ووا وحر النساء والذور والحياة وصيد من بنا أنا الذي أحب بينا على الساعة العارب أنا ما زلت عير ماء الأذياء والأهداء واذت تناقي مم \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a seed\n",
    "seed = ' أهكذا أبدا تمضي أمانينا  نطوي الحياة وليل الموت يطوينا  تجري بنا سفن الأعمار ماخرة  بحر الوجود ولا ' # has to be 100 chars\n",
    "pattern = [char_to_int[char] for char in seed]\n",
    "\n",
    "print(\"Seed:\")\n",
    "print( \"\\\"\", seed, \"\\\"\\n\")\n",
    "# generate characters\n",
    "for i in range(140):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
